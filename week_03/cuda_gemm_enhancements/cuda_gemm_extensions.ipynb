{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtzFkEZSkR_m",
        "outputId": "a212aa9e-5231-45d6-9a6e-0c407f05ad6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Sep 24 21:12:44 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gemm_xt.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <vector>\n",
        "#include <random>\n",
        "#include <string>\n",
        "#include <cmath>\n",
        "#include <iostream>\n",
        "\n",
        "#define CHECK_CUDA(x) do { cudaError_t e=(x); if(e!=cudaSuccess){ \\\n",
        "  fprintf(stderr,\"CUDA %s:%d: %s\\n\",__FILE__,__LINE__,cudaGetErrorString(e)); exit(1);} } while(0)\n",
        "\n",
        "enum Transpose : int { N=0, T=1 };\n",
        "\n",
        "// row-major index\n",
        "__host__ __device__ inline int idx(int r, int c, int ld) { return r*ld + c; }\n",
        "\n",
        "// logical reads for op(A) and op(B) using stored dims\n",
        "__host__ __device__ inline float read_opA(const float* A, int i, int k,\n",
        "                                          int mA, int nA, Transpose tA) {\n",
        "  // stored A is mA x nA (row-major)\n",
        "  // if tA==N: opA(i,k) = A[i,k]\n",
        "  // if tA==T: opA(i,k) = A^T[i,k] = A[k,i]\n",
        "  return (tA==N) ? A[idx(i,k,nA)] : A[idx(k,i,nA)];\n",
        "}\n",
        "\n",
        "__host__ __device__ inline float read_opB(const float* B, int k, int j,\n",
        "                                          int mB, int nB, Transpose tB) {\n",
        "  // stored B is mB x nB\n",
        "  // if tB==N: opB(k,j) = B[k,j]\n",
        "  // if tB==T: opB(k,j) = B^T[k,j] = B[j,k]\n",
        "  return (tB==N) ? B[idx(k,j,nB)] : B[idx(j,k,nB)];\n",
        "}\n",
        "\n",
        "// ---------------------- NAIVE KERNEL ----------------------\n",
        "__global__ void gemm_naive_inplace(const float* __restrict__ A,\n",
        "                                   const float* __restrict__ B,\n",
        "                                   float* __restrict__ C, // in-place\n",
        "                                   int m, int n, int K,\n",
        "                                   int mA, int nA, int mB, int nB,\n",
        "                                   Transpose tA, Transpose tB,\n",
        "                                   float alpha, float beta) {\n",
        "  int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (row >= m || col >= n) return;\n",
        "\n",
        "  float acc = 0.f;\n",
        "  for (int k=0; k<K; ++k) {\n",
        "    float a = read_opA(A, row, k, mA, nA, tA);\n",
        "    float b = read_opB(B,  k, col, mB, nB, tB);\n",
        "    acc = fmaf(a, b, acc);\n",
        "  }\n",
        "  C[idx(row,col,n)] = alpha*acc + beta*C[idx(row,col,n)];\n",
        "}\n",
        "\n",
        "// ---------------------- TILED KERNEL ----------------------\n",
        "template<int BLOCK>\n",
        "__global__ void gemm_tiled_inplace(const float* __restrict__ A,\n",
        "                                   const float* __restrict__ B,\n",
        "                                   float* __restrict__ C,\n",
        "                                   int m, int n, int K,\n",
        "                                   int mA, int nA, int mB, int nB,\n",
        "                                   Transpose tA, Transpose tB,\n",
        "                                   float alpha, float beta) {\n",
        "  __shared__ float As[BLOCK][BLOCK+1];\n",
        "  __shared__ float Bs[BLOCK][BLOCK+1];\n",
        "\n",
        "  int row = blockIdx.y * BLOCK + threadIdx.y;\n",
        "  int col = blockIdx.x * BLOCK + threadIdx.x;\n",
        "\n",
        "  float acc = 0.f;\n",
        "  int tiles = (K + BLOCK - 1) / BLOCK;\n",
        "\n",
        "  for (int t=0; t<tiles; ++t) {\n",
        "    int k0 = t*BLOCK;\n",
        "\n",
        "    // load tile of op(A): rows=row, cols=k0..k0+BLOCK-1\n",
        "    int aCol = k0 + threadIdx.x;\n",
        "    As[threadIdx.y][threadIdx.x] =\n",
        "      (row<m && aCol<K) ? read_opA(A, row, aCol, mA, nA, tA) : 0.f;\n",
        "\n",
        "    // load tile of op(B): rows=k0..k0+BLOCK-1, cols=col\n",
        "    int bRow = k0 + threadIdx.y;\n",
        "    Bs[threadIdx.y][threadIdx.x] =\n",
        "      (bRow<K && col<n) ? read_opB(B, bRow, col, mB, nB, tB) : 0.f;\n",
        "\n",
        "    __syncthreads();\n",
        "    #pragma unroll\n",
        "    for (int kk=0; kk<BLOCK; ++kk) {\n",
        "      acc = fmaf(As[threadIdx.y][kk], Bs[kk][threadIdx.x], acc);\n",
        "    }\n",
        "    __syncthreads();\n",
        "  }\n",
        "\n",
        "  if (row<m && col<n) {\n",
        "    float c_old = C[idx(row,col,n)];\n",
        "    C[idx(row,col,n)] = alpha*acc + beta*c_old;\n",
        "  }\n",
        "}\n",
        "\n",
        "// ---------------------- Host driver ----------------------\n",
        "struct Args {\n",
        "  int m=1024, n=1024, k=1024;\n",
        "  Transpose tA=N, tB=N;\n",
        "  float alpha=1.f, beta=1.f;\n",
        "  int repeat=20;\n",
        "  std::string kernel=\"tiled\";\n",
        "  int block=32;\n",
        "  int seed=42;\n",
        "};\n",
        "\n",
        "Args parse_args(int argc, char** argv){\n",
        "  Args a;\n",
        "  for(int i=1;i<argc;++i){\n",
        "    std::string s(argv[i]); auto next=[&](int&i){return std::string(argv[++i]);};\n",
        "    if(s==\"--m\") a.m=std::stoi(next(i));\n",
        "    else if(s==\"--n\") a.n=std::stoi(next(i));\n",
        "    else if(s==\"--k\") a.k=std::stoi(next(i));\n",
        "    else if(s==\"--alpha\") a.alpha=std::stof(next(i));\n",
        "    else if(s==\"--beta\") a.beta=std::stof(next(i));\n",
        "    else if(s==\"--repeat\") a.repeat=std::stoi(next(i));\n",
        "    else if(s==\"--kernel\") a.kernel=next(i);\n",
        "    else if(s==\"--block\") a.block=std::stoi(next(i));\n",
        "    else if(s==\"--tA\") { char c=next(i)[0]; a.tA=(c=='T'||c=='t')?T:N; }\n",
        "    else if(s==\"--tB\") { char c=next(i)[0]; a.tB=(c=='T'||c=='t')?T:N; }\n",
        "    else if(s==\"--seed\") a.seed=std::stoi(next(i));\n",
        "  }\n",
        "  return a;\n",
        "}\n",
        "\n",
        "static double gflops_gemm(long long m,long long n,long long K,double ms){\n",
        "  double flops = 2.0*m*n*K + 2.0*m*n; // include axpby\n",
        "  return flops/(ms*1e6);\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv){\n",
        "  Args args = parse_args(argc, argv);\n",
        "  int m=args.m, n=args.n, K=args.k;\n",
        "\n",
        "  // stored shapes according to transpose flags\n",
        "  int mA = (args.tA==N)? m : K;\n",
        "  int nA = (args.tA==N)? K : m;\n",
        "  int mB = (args.tB==N)? K : n;\n",
        "  int nB = (args.tB==N)? n : K;\n",
        "\n",
        "  // host buffers\n",
        "  std::mt19937 rng(args.seed);\n",
        "  std::uniform_real_distribution<float> dist(-1.f,1.f);\n",
        "  std::vector<float> A(mA*nA), B(mB*nB), C(m*n), C_ref(m*n);\n",
        "  for (auto& x:A) x=dist(rng);\n",
        "  for (auto& x:B) x=dist(rng);\n",
        "  for (auto& x:C) x=dist(rng);\n",
        "  C_ref = C; // <-- FIX: just clone C\n",
        "\n",
        "  // CPU reference (in-place)\n",
        "  auto opA = [&](int i,int kk)->float{\n",
        "    return (args.tA==N) ? A[idx(i,kk,nA)] : A[idx(kk,i,nA)];\n",
        "  };\n",
        "  auto opB = [&](int kk,int j)->float{\n",
        "    return (args.tB==N) ? B[idx(kk,j,nB)] : B[idx(j,kk,nB)];\n",
        "  };\n",
        "  for(int i=0;i<m;++i){\n",
        "    for(int j=0;j<n;++j){\n",
        "      double acc=0.0;\n",
        "      for(int kk=0; kk<K; ++kk) acc += (double)opA(i,kk) * (double)opB(kk,j);\n",
        "      C_ref[idx(i,j,n)] = (float)(args.alpha*acc + args.beta*C_ref[idx(i,j,n)]);\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // device buffers\n",
        "  float *dA,*dB,*dC;\n",
        "  CHECK_CUDA(cudaMalloc(&dA,sizeof(float)*A.size()));\n",
        "  CHECK_CUDA(cudaMalloc(&dB,sizeof(float)*B.size()));\n",
        "  CHECK_CUDA(cudaMalloc(&dC,sizeof(float)*C.size()));\n",
        "  CHECK_CUDA(cudaMemcpy(dA,A.data(),sizeof(float)*A.size(),cudaMemcpyHostToDevice));\n",
        "  CHECK_CUDA(cudaMemcpy(dB,B.data(),sizeof(float)*B.size(),cudaMemcpyHostToDevice));\n",
        "  CHECK_CUDA(cudaMemcpy(dC,C.data(),sizeof(float)*C.size(),cudaMemcpyHostToDevice));\n",
        "\n",
        "  // launch config\n",
        "  dim3 block(16,16);\n",
        "  dim3 grid((n+block.x-1)/block.x, (m+block.y-1)/block.y);\n",
        "\n",
        "  cudaEvent_t start,stop; CHECK_CUDA(cudaEventCreate(&start)); CHECK_CUDA(cudaEventCreate(&stop));\n",
        "\n",
        "  auto run_naive = [&](){\n",
        "    gemm_naive_inplace<<<grid,block>>>(dA,dB,dC,m,n,K,mA,nA,mB,nB,args.tA,args.tB,args.alpha,args.beta);\n",
        "  };\n",
        "  auto run_tiled = [&](){\n",
        "    if(args.block==16){\n",
        "      dim3 b(16,16), g((n+15)/16,(m+15)/16);\n",
        "      gemm_tiled_inplace<16><<<g,b>>>(dA,dB,dC,m,n,K,mA,nA,mB,nB,args.tA,args.tB,args.alpha,args.beta);\n",
        "    } else {\n",
        "      dim3 b(32,32), g((n+31)/32,(m+31)/32);\n",
        "      gemm_tiled_inplace<32><<<g,b>>>(dA,dB,dC,m,n,K,mA,nA,mB,nB,args.tA,args.tB,args.alpha,args.beta);\n",
        "    }\n",
        "  };\n",
        "\n",
        "  // warmup\n",
        "  if(args.kernel==\"naive\") run_naive(); else run_tiled();\n",
        "  CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "  // time\n",
        "  CHECK_CUDA(cudaEventRecord(start));\n",
        "  for(int r=0;r<args.repeat;++r){\n",
        "    if(args.kernel==\"naive\") run_naive(); else run_tiled();\n",
        "  }\n",
        "  CHECK_CUDA(cudaEventRecord(stop)); CHECK_CUDA(cudaEventSynchronize(stop));\n",
        "  float ms=0.f; CHECK_CUDA(cudaEventElapsedTime(&ms,start,stop)); ms/=args.repeat;\n",
        "\n",
        "  // copy back and check\n",
        "  CHECK_CUDA(cudaMemcpy(C.data(),dC,sizeof(float)*C.size(),cudaMemcpyDeviceToHost));\n",
        "  float mad=0.f; for(size_t i=0;i<C.size();++i) mad=fmaxf(mad, fabsf(C[i]-C_ref[i]));\n",
        "\n",
        "  std::cout<<\"Kernel=\"<<args.kernel<<\" block=\"<<args.block\n",
        "           <<\"  m=\"<<m<<\" n=\"<<n<<\" k=\"<<K\n",
        "           <<\"  tA=\"<<(args.tA==N?'N':'T')<<\" tB=\"<<(args.tB==N?'N':'T')\n",
        "           <<\"  alpha=\"<<args.alpha<<\" beta=\"<<args.beta<<\"\\n\";\n",
        "  std::cout<<\"Avg time \"<<ms<<\" ms   Throughput \"<<gflops_gemm(m,n,K,ms)<<\" GFLOP/s\\n\";\n",
        "  std::cout<<\"Max |C_gpu - C_ref| = \"<<mad<<\"\\n\";\n",
        "\n",
        "  CHECK_CUDA(cudaFree(dA)); CHECK_CUDA(cudaFree(dB)); CHECK_CUDA(cudaFree(dC));\n",
        "  CHECK_CUDA(cudaEventDestroy(start)); CHECK_CUDA(cudaEventDestroy(stop));\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_0nBuTbkliR",
        "outputId": "375c01eb-3768-46c9-c3d9-1b9e48f2fc40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting gemm_xt.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O3 -std=c++17 gemm_xt.cu -o gemm_xt\n"
      ],
      "metadata": {
        "id": "wwI6c1DFkrj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile softmax_online.c\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <time.h>\n",
        "\n",
        "// Algorithm 3 (online normalizer):\n",
        "// m0=-inf, d0=0\n",
        "// for j in 1..V:\n",
        "//   mj = max(m_{j-1}, x_j)\n",
        "//   d  = d * exp(m_{j-1}-mj) + exp(x_j - mj)\n",
        "// Then y_i = exp(x_i - mV) / dV\n",
        "void softmax_online(const float* x, float* y, int V) {\n",
        "    if (V<=0) return;\n",
        "    float m = -INFINITY;\n",
        "    float d = 0.0f;\n",
        "    for (int j=0; j<V; ++j) {\n",
        "        float xj = x[j];\n",
        "        float mj = fmaxf(m, xj);\n",
        "        // adjust normalizer to new mj and add current term\n",
        "        float d_scaled = (m==-INFINITY) ? 0.0f : d * expf(m - mj);\n",
        "        d = d_scaled + expf(xj - mj);\n",
        "        m = mj;\n",
        "    }\n",
        "    // final normalization\n",
        "    for (int i=0; i<V; ++i) {\n",
        "        y[i] = expf(x[i] - m) / d;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Reference \"safe\" softmax (two-pass): subtract max first, then sum exp\n",
        "void softmax_ref(const float* x, float* y, int V) {\n",
        "    float m = -INFINITY;\n",
        "    for (int i=0;i<V;++i) if (x[i]>m) m=x[i];\n",
        "    double sum = 0.0;\n",
        "    for (int i=0;i<V;++i) sum += exp((double)x[i]-m);\n",
        "    for (int i=0;i<V;++i) y[i] = (float)(exp((double)x[i]-m)/sum);\n",
        "}\n",
        "\n",
        "static float max_abs_diff(const float* a,const float* b,int n){\n",
        "    float m=0.f; for(int i=0;i<n;++i){ float d=fabsf(a[i]-b[i]); if(d>m)m=d; } return m;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv){\n",
        "    int V = (argc>1)? atoi(argv[1]) : 4096;\n",
        "    int trials = (argc>2)? atoi(argv[2]) : 64;\n",
        "    unsigned seed = (argc>3)? (unsigned)atoi(argv[3]) : 42u;\n",
        "    srand(seed);\n",
        "\n",
        "    float* x = (float*)malloc(sizeof(float)*V);\n",
        "    float* y1= (float*)malloc(sizeof(float)*V);\n",
        "    float* y2= (float*)malloc(sizeof(float)*V);\n",
        "    if(!x||!y1||!y2){ fprintf(stderr,\"oom\\n\"); return 1; }\n",
        "\n",
        "    double worst=0.0, avg=0.0;\n",
        "    for(int t=0;t<trials;++t){\n",
        "        // random inputs over a wide dynamic range to stress numerical stability\n",
        "        for(int i=0;i<V;++i){\n",
        "            float r = ((float)rand()/(float)RAND_MAX); // [0,1]\n",
        "            float z = 40.f*(2.f*r-1.f);                // [-40,40]\n",
        "            x[i]=z;\n",
        "        }\n",
        "        softmax_online(x,y1,V);\n",
        "        softmax_ref(x,y2,V);\n",
        "        float mad = max_abs_diff(y1,y2,V);\n",
        "        worst = fmax(worst, mad);\n",
        "        avg += mad;\n",
        "    }\n",
        "    avg/=trials;\n",
        "    printf(\"OK. V=%d trials=%d  max_abs_diff=%.3e  avg_abs_diff=%.3e\\n\", V, trials, worst, avg);\n",
        "\n",
        "    // small sanity: y sums ~ 1\n",
        "    double s=0.0; for(int i=0;i<V;++i) s += y1[i];\n",
        "    printf(\"sum(y_online) = %.9f\\n\", s);\n",
        "\n",
        "    free(x); free(y1); free(y2);\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKjVN3lDk6fv",
        "outputId": "4748d0ee-663e-455f-a9ee-f3de3b5014fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing softmax_online.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcc -O3 -std=c11 softmax_online.c -lm -o softmax_online\n",
        "!./softmax_online 4096 100\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrCj7JcUk-US",
        "outputId": "4eb40cc5-a0a7-49e7-fdff-80169b7020aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK. V=4096 trials=100  max_abs_diff=4.098e-08  avg_abs_diff=1.501e-08\n",
            "sum(y_online) = 1.000000735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GfM3AFY4k_Zv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}