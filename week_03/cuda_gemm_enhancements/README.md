GPU Programming â€“ Week 3 README

This repo/notebook contains implementations and benchmarks for the Week-3 assignment:

In-place GEMM with transpose flags

HBM-aware tiled GEMM (shared memory)

Register-tiled GEMM (shared + micro-tiles)

Online normalizer softmax (Algorithm 3) from A Fast and Accurate Replacement for Softmax (arXiv:1805.02867)

All CUDA GEMM variants implement:

ğ¶
â†
ğ›¼
â‹…
op
â¡
(
ğ´
)
op
â¡
(
ğµ
)
+
ğ›½
â‹…
ğ¶
,
op
â¡
(
ğ‘‹
)
âˆˆ
{
ğ‘‹
,
â€…â€Š
ğ‘‹
âŠ¤
}
.
Câ†Î±â‹…op(A)op(B)+Î²â‹…C,op(X)âˆˆ{X,X
âŠ¤
}.

No high-level CUDA libraries (no cuBLAS/cuDNN) are used.

Contents

gemm_xt.cu â€“ In-place GEMM with optional transpose (tA, tB) + tiled shared-memory kernel and a naÃ¯ve baseline.

gemm_xt_regtiling.cu â€“ Register-tiled GEMM (shared tiles + per-thread micro-tiles).

softmax_online.c â€“ Online normalizer softmax (Algorithm 3) + parity tests vs a safe two-pass reference.

Quick start (Colab)

Enable GPU â†’ Runtime â†’ Change runtime type â†’ GPU.

Build

!nvcc -O3 -std=c++17 gemm_xt.cu -o gemm_xt
!nvcc -O3 -std=c++17 gemm_xt_regtiling.cu -o gemm_xt_regtiling
!gcc  -O3 -std=c11  softmax_online.c -lm -o softmax_online


Run (examples below).

GEMM: CLI & usage

Both CUDA binaries accept the same flags:

--m, --n, --k : output is mÃ—n, reduction dim k

--tA N|T, --tB N|T : use op(A) = A or Aáµ€, op(B) = B or Báµ€

--alpha <float>, --beta <float>

--repeat <int> : timing iterations (averaged)

--seed <int> : host RNG for inputs

For gemm_xt.cu only:

--kernel naive|tiled

--block 16|32 : shared tile size for the tiled kernel

For gemm_xt_regtiling.cu only:

--kernel reg|naive (register-tiling vs naÃ¯ve)

Shapes (stored vs logical)

If tA=N: A stored as mÃ—k; if tA=T: A stored as kÃ—m.

If tB=N: B stored as kÃ—n; if tB=T: B stored as nÃ—k.

C is always stored mÃ—n. (All arrays row-major.)

Sanity run (all transpose combos)
# Tiled (shared memory) â€” HBM-aware
!./gemm_xt --m 512 --n 384 --k 256 --tA N --tB N --alpha 1.2 --beta 0.7 --kernel tiled --block 32 --repeat 20
!./gemm_xt --m 512 --n 384 --k 256 --tA T --tB N --alpha 0.9 --beta 0.1 --kernel tiled --block 32 --repeat 20
!./gemm_xt --m 512 --n 384 --k 256 --tA N --tB T --alpha 1.0 --beta 0.0 --kernel tiled --block 32 --repeat 20
!./gemm_xt --m 512 --n 384 --k 256 --tA T --tB T --alpha 1.0 --beta 1.0 --kernel tiled --block 32 --repeat 20


Youâ€™ll see:

Max |C_gpu - C_ref| â‰ˆ 1e-3â€“1e-4 for random inputs (float32).

Average time and GFLOP/s.

NaÃ¯ve vs Tiled vs Register-Tiled
# NaÃ¯ve baseline
!./gemm_xt --m 1024 --n 1024 --k 1024 --tA N --tB N --alpha 1 --beta 1 --kernel naive --repeat 10

# Tiled shared-memory kernel
!./gemm_xt --m 1024 --n 1024 --k 1024 --tA N --tB N --alpha 1 --beta 1 --kernel tiled --block 32 --repeat 20

# Register-tiling (shared + micro-tiles)
!./gemm_xt_regtiling --m 1024 --n 1024 --k 1024 --tA N --tB N --alpha 1 --beta 1 --kernel reg --repeat 20

What each CUDA kernel does
NaÃ¯ve

Each thread computes one C[i,j].

Reads a full row of op(A) and column of op(B) from global memory â†’ simple, bandwidth-hungry.

Tiled (shared memory)

Block computes a BLOCKÃ—BLOCK output tile.

Cooperatively loads BLOCKÃ—BLOCK tiles of op(A) and op(B) into shared memory, reusing each value BLOCK times.

--block {16,32}; +1 padding in shared arrays reduces bank conflicts.

Fused epilogue: in-place C[i,j] = Î±Â·acc + Î²Â·C[i,j].

Register-tiled (shared + micro-tiles)

Block computes a large tile (e.g., 64Ã—64), traversing K in chunks (e.g., KT=16).

Each thread accumulates a TMÃ—TN patch (default 2Ã—2) in registers.

Higher arithmetic intensity and fewer global loads than the plain tiled kernel.

Default config in gemm_xt_regtiling.cu:

Block tile BLOCK_MÃ—BLOCK_N = 64Ã—64

K tile KT = 16

Micro-tile TMÃ—TN = 2Ã—2

Threads per block = (BLOCK_N/TN) Ã— (BLOCK_M/TM) = 32 Ã— 32 = 1024

Want to explore? Recompile with different template constants (e.g., 128Ã—64Ã—8, TMÃ—TN=4Ã—2) and measure.

Online softmax (Algorithm 3)

Implements the single-pass, numerically stable softmax with on-the-fly max and normalizer updates.

Build & test

!gcc -O3 -std=c11 softmax_online.c -lm -o softmax_online
!./softmax_online 4096 100


Expected output

Very small max_abs_diff and avg_abs_diff vs a safe two-pass reference.

sum(y) â‰ˆ 1.0 (within ~1e-6â€“1e-7).

Algorithm (pseudocode)

m = -inf
d = 0
for xj in x:
    mj = max(m, xj)
    d  = d * exp(m - mj) + exp(xj - mj)
    m  = mj
y_i = exp(x_i - m) / d


Reference: A Fast and Accurate Replacement for Softmax (arXiv:1805.02867), Algorithm 3.

Design notes & pitfalls

In-place semantics: We always read the original C and write back Î±Â·acc + Î²Â·C. No extra D buffer is allocated.

Transpose handling: Done at globalâ†’shared load time via helper indexers; ensures coalesced reads for both N and T.

Numerical accuracy: Float32 accumulations; differences vs CPU double-accumulation are expected at ~1e-3 for large random matrices.

Block sizes & occupancy:

Tiled: --block 32 often best; try 16 on smaller GPUs.

Register-tiling: 64Ã—64 with 1024 threads is a good, safe defaultâ€”benchmark on your GPU.

Bank conflicts: Shared tiles are padded (+1) in one dimension.

Repro tips

Use larger sizes (e.g., 2048+) to approach steady-state bandwidth and higher GFLOP/s.

Increase --repeat for stable timing.

Fix the RNG --seed to compare kernels apples-to-apples.
